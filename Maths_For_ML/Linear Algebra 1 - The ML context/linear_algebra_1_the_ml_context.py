# -*- coding: utf-8 -*-
"""Linear Algebra 1 The ML Context.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RwX3HBOrPOFR_c_h78rpqXyArTYfFWBq

## Content

- Introduction to a real life problem
  - Terminologies
  - Visualizing the data
- More ML Examples
- Process of Building ML Algorithm
- Coordinate Geomtery
  - Slope intercept form of Line
  - General form of line
  - Parallel lines
  - Perpedicular lines
  - More than 2 dimensions
- Halfspaces Intro
- Use case: Orange vs Tangerine

## Introduction to a real life problem

let's go through a small video: https://www.youtube.com/shorts/pfnH5lJbMLM

Notice how each fish is getting sorted into different buckets.

Inherently, we know that there must be a scanner that analyses certain parameters.

<br>

**What do you think are the parameters based on which the sorting works?**

- Weight
- Size
- Appearance
  - Colour
  - Shape and so on

To solve this automation task we would need the scanner to collect the data send it somewhere (essentially a model), then the model make a prediction using the data and will return the output to the seperator.

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/045/969/original/Screenshot_2023-09-04_at_3.19.25_PM.png?1693820978)

#### **Terminologies**

Let say that we are given a **Labelled dataset** ie a table as shown below

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/045/972/original/Screenshot_2023-09-04_at_3.28.03_PM.png?1693821495)

**What do you call these parameters that help us determine the fish type?**
- It is called **features**.
- They are also referred to as **Independent variable**
- We can record values of each features using a sensor

**And this different type of fishes is known as?**
- These are known as **targets** or is also called as **labels** or **Dependent Variable**.
- It tells us which type does a given row belong to

Given a datapoint our job is to predict it's respective Label

#### **Visualizing the data**

So let's take the data that we have and plot them

- Since we have 3 features with us let's pick up 2 features which is height and width and create a scatter plot to understand what is their replationship with each other as well as their combined relationship with the target variable.
- This is a 2-D plot
- As we can see that we can have infinte number lines between green points and the Blue points
<br>

**The question is which is the best line that we should pick that will help us speerate Blue points from green points?**
- This is the question that we will be solving in this entier module.

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/045/983/original/Screenshot_2023-09-04_at_3.58.54_PM.png?1693823360)

## More ML Examples

##### **IPL Win Prediction**

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/045/986/original/Screenshot_2023-09-04_at_4.04.22_PM.png?1693823691)

##### **T-Shirt size prediction**

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/045/988/original/Screenshot_2023-09-04_at_4.12.58_PM.png?1693824193)

## Process of Builing ML Algorithm

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/045/993/original/Screenshot_2023-09-04_at_4.33.35_PM.png?1693825433)

## **Coordinate Geomtery**

### Basics of Lines

The basic equation of Line is

**y = mx + c**

This is also referred to as **Slope intercept equation** of a Line

- **x** is input and is an indepedent variable
- **m** is slope of the line ie the angle it makes with the x axis
- **c** is the y-intercept
- **y** is the output and is a dependant variable ie it is dependent on x given that we keep m and c contant

For a given point ($x_1$,$y_1$) if it lies on our line $L_1$ it should satisfy the equation of our line and the equation would be

**$y_1=mx_1+C$**

- The slope **m** can be calculated by calculating **$tan \ \theta$ = $\frac{y_2 - y_1}{x_2 - x_1}$**, where $(x_1, y_1) \ (x_2, y_2)$ are two coordinates on the line.

- Range of $tan \ \theta$ is from -$\infty$ to $\infty$
- $tan(90)$= $\infty$ due which there is a limitation which is using the slope intercept form we cannot create a line which is 90 degrees to x-axis and to overcome this we use general form of line

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/046/001/original/Screenshot_2023-09-04_at_4.51.22_PM.png?1693826495)

### **General form of Line**

We might have already studied this form which is

**Ax+By+c=0**

- Now here instead of using A and B we are using $W_1$ and $W_2$
  - In the ML context they are referred to as `weights` that the computer learns
- Instead of x and y we use $x_1$ and $x_2$ they both mean the same thing that it has 2 Dimensions
  - In the ML context they are referred to as `features` which we saw above line legth, weight,etc.
- C is represented as $W_0$
  - In the ML context this is referred to as `Bias`
- We are essentally changing the way it is represented that is because we follow this form of representation in Machine learning and the terminologies changes when we go to the Machine learning side of representation

We can convert general form of line into slope intercept form.

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/046/026/original/Screenshot_2023-09-04_at_5.20.00_PM.png?1693828216)

Now using the equation of m that we got after converting general form into slope intercept form
- if we want to have a perpedicular line to x axis we know that m has to be infite and we can do that by equating $w_2$ to 0 and we can see that we have a line that is perpedicular to x axis

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/046/032/original/Screenshot_2023-09-04_at_5.39.03_PM.png?1693829486)

Now if we want the y intercept to be equal to 1 using the new equation of the intecept which is

$-\frac{w_0}{w_2}$ where if $w_2=0.8$ then $w_0$ should be equal to -0.8.

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/046/031/original/Screenshot_2023-09-04_at_5.38.03_PM.png?1693829298)

### Parallel lines

If 2 lines are parallel to each other then they will have same angle with the x-axis which means that both the lines will have same slopes

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/046/033/original/Screenshot_2023-09-04_at_5.50.27_PM.png?1693830043)

### Perpendicular Lines

- The product of the slopes of 2 perpendicular lines is equal to -1
- if slope of line1 is $m_1$ and slope of line2 is $m_2$ then the formula to represent slope of 2 perpendicular lines is
$m_1*m_2=-1$ which is equal to $m_1=-\frac{1}{m_2}$
- using the general form we have

$-\frac{w_1}{w_2}*-\frac{w_3}{w_4}=-1$

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/046/036/original/Screenshot_2023-09-04_at_5.54.55_PM.png?1693830314)

### More than 2 dimensions

How can we take the general form of line and expand it to higher dimensions that is till n dimensions?

So this is called as the **Generalized equation of a line**

- 2D it is a Line
- 3D it is a plane
- 4D it is 4D hyperplane
- nD it is a nD hyperplane

The term hyperplanes because we cannot visualize beyound 3D

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/046/038/original/Screenshot_2023-09-04_at_6.07.51_PM.png?1693831088)

## Halfspaces Intro

When we draw a line we can see that we are esentially diving the 2D space into 2 differnet spaces
- one which lies above the line
- other is the one that lies below the line

Similarly in 3D we can divide the space into 2 parts using a Plane
- One which lines above the plane
- other is one that lies below the plane

And we can extend this concept till n dimensions

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/046/040/original/Screenshot_2023-09-04_at_6.16.09_PM.png?1693831587)

**Why do we care about halfspaces?**
- Whenever we are asked to classify a new datapoint we just plot the given datapoint and see in which half space does it belong to using which we can classify the given point.
- More on this we'll be clearing in the next class

## Use case: Orange vs Tangerine

Now suppose this is our usecase, to classify the fruits based on their distinct characteristics
"""

!wget https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/030/385/original/fruits.csv?1680749521 -O fruits.csv

import pandas as pd
df = pd.read_csv('fruits.csv')
df.head(10)

"""Q. What are the features in this dataset?
- Weight, Diameter and color

Q. What do you call the column **fruit**
- Target variable

Q. How many classes do we have and which type of ML problem it is?
- We have 2 classes and it is a binary classification task

Let's plot and see what the data looks like
"""

import seaborn as sns
sns.scatterplot(x='weight', y = 'diameter', hue='fruit', data=df)

"""Now, if I have to divide the oranges and tangerine , what would be a good line to separate them?

![](https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/046/073/original/Screenshot_2023-09-04_at_8.12.21_PM.png?1693838553)

Notice, all the three lines separate the orange and blue points

**How do we find the best line from these?**

- If you notice, the clusters can be defined as
  - High diameter, low weight: Orange
  - Low diameter, high weight: Tangerine

Q. Based on this, which among the 3 would be a good decision boundary?
  - Red

Notice, we are always given the x value ie we'll be getting this from the dataset. So now, we need to find the line's m and c?

Let's try to find the value of these parameters and classify the above points using a small simulation
"""

import numpy as np
import matplotlib.pyplot as plt
from ipywidgets import interactive

def plot_line(m, c):
    x = np.linspace(2, 5, num=100)  # generate x values
    y = m*x + c  # calculate y values
    # plt.plot(x, y)  # plot the line
    sns.scatterplot(x='weight', y = 'diameter', hue='fruit', data=df)
    plt.plot(x, y)  # plot the line
    # plt.ylim(-50, 50)  # set y-axis limits
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title(f'y = {m}x + {c}')
    plt.show()

interactive_plot = interactive(plot_line, m=(-1, 5, 0.1), c=(-5, 5, 1))
interactive_plot

"""Notice, for
- m = 1.7
- c = -2

We are able to find a good enough line to separate both the orange and blue poitns clusters

But do we need to always randomly try out the values to find the parameters?

This is where ML comes in. It helps us to calculate these unknown parameters to find the best-fitting line, given the values of x and we'll learn about this in the upcoming class.

"""